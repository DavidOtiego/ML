{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1728\n",
      "Number of features: 6\n",
      "Feature column(s):-\n",
      "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
      "Target column: expected_output\n",
      "\n",
      "Feature values:-\n",
      "  buying  maint doors persons lug_boot safety\n",
      "0  vhigh  vhigh     2       2    small    low\n",
      "1  vhigh  vhigh     2       2    small    med\n",
      "2  vhigh  vhigh     2       2    small   high\n",
      "3  vhigh  vhigh     2       2      med    low\n",
      "4  vhigh  vhigh     2       2      med    med\n",
      "Processed feature columns (21):-\n",
      "['buying_high', 'buying_low', 'buying_med', 'buying_vhigh', 'maint_high', 'maint_low', 'maint_med', 'maint_vhigh', 'doors_2', 'doors_3', 'doors_4', 'doors_5more', 'persons_2', 'persons_4', 'persons_more', 'lug_boot_big', 'lug_boot_med', 'lug_boot_small', 'safety_high', 'safety_low', 'safety_med']\n",
      "Training set: 1296 samples\n",
      "Test set: 432 samples\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety','expected_output']\n",
    "\n",
    "dataset = pandas.read_csv(url, names=names)\n",
    "\n",
    "# Not Sure\n",
    "#data = numpy.array(dataset)  #convert array to numpy type array\n",
    "#x_train, x_test = train_test_split(data,test_size=0.20)\n",
    "\n",
    "#Info on dataset\n",
    "n_rows = dataset.shape[0]\n",
    "n_features = dataset.shape[1] - 1\n",
    "print( \"Total number of rows: {}\".format(n_rows))\n",
    "print( \"Number of features: {}\".format(n_features))\n",
    "\n",
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(dataset.columns[:-1])  # all columns but last is target/label\n",
    "target_col = dataset.columns[-1]  # last column is the target/label\n",
    "print( \"Feature column(s):-\\n{}\".format(feature_cols))\n",
    "print( \"Target column: {}\".format(target_col))\n",
    "\n",
    "X_all = dataset[feature_cols]  # feature values for all cars\n",
    "y_all = dataset[target_col]  # corresponding targets/labels\n",
    "print (\"\\nFeature values:-\")\n",
    "print( X_all.head())  # print the first 5 rows\n",
    "\n",
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    #pd refers to pandas library\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        #if col_data.dtype == object:\n",
    "            #col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'safety' => 'safety_LOW', 'safety_MED', 'safety_HIGH'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "#prepare the dataset before processing it, non-numeric values need to be converted\n",
    "X_all = preprocess_features(X_all)\n",
    "print( \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns)))\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.75)\n",
    "\n",
    "print (\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Test set: {} samples\".format(X_test.shape[0]))\n",
    "# Note: Validation set can be extracted from training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
